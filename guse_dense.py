# -*- coding: utf-8 -*-
"""Copy of GUSE-Dense

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1eDlGmUnIqlFjp9GVIiq2M64Cpl44q8vO
"""

import pandas as pd
import numpy as np

df = pd.read_csv('combined-set.csv')
df.shape

df.head()

#Identifying null value in every column
df.isnull().sum()

#Joining 'title' and 'selftext' and create new column 'original_text'
df["original_text"] = df["title"].str.cat(df["selftext"],sep=" ")

#Select column needed
df = df[['is_suicide', 'original_text']]

df.head()

#create new column named 'label' that classify 0 == depressed, 1 == suicide
df["label"] = df["is_suicide"].apply(lambda x: "depressed" if x < 1 else "suicidal")

df.head()

"""**PREPROCESSING PART**"""

import re
import string

#Remove Numeric
def remove_numeric(text):
    return re.sub(r'\d+', '', text)

#Remove â€™t from the text
def remove_t(text):
    return text.replace("â€™t", "")

#Remove extra whitespace
def remove_extra_whitespace(text):
    return " ".join(text.split())

#Remove URL
def remove_url(text):
    return re.sub(r'https?://\S+|www\.\S+', '', text)

#Remove Punctuation and Special Character
def remove_punctuation(text):
    return text.translate(text.maketrans("", "", string.punctuation))

import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
nltk.download('punkt')
#Remove Stopwords
def remove_stopwords(text):
    stop_words = set(stopwords.words("english"))
    words = word_tokenize(text)
    filtered_text = [w for w in words if not w in stop_words]
    return " ".join(filtered_text)

from nltk.stem import WordNetLemmatizer
nltk.download('wordnet')

def lemmatize_text(text):
    lemmatizer = WordNetLemmatizer()
    words = word_tokenize(text)
    lemmatized_text = [lemmatizer.lemmatize(word) for word in words]
    return " ".join(lemmatized_text)

import nltk
nltk.download('stopwords')
nltk.download('wordnet')
nltk.download('omw-1.4')

df["original_text"] = df["original_text"].str.lower() #lowercase
df["original_text"] = df["original_text"].apply(remove_numeric)
df["original_text"] = df["original_text"].apply(remove_punctuation)
df["original_text"] = df["original_text"].apply(remove_t)
df["original_text"] = df["original_text"].apply(remove_extra_whitespace)
df["original_text"] = df["original_text"].apply(remove_url)
df["original_text"] = df["original_text"].apply(remove_stopwords)
df["original_text"] = df["original_text"].apply(lemmatize_text)

df["original_text"].to_csv('clean.csv')

df.head()

df["original_text"] = df["original_text"].apply(lemmatize_text)
df["lemmatized_words"] = df["original_text"].str.split()
print(df["lemmatized_words"])

# df.to_csv('cleaned.csv', encoding='utf-8', index=False)

import tensorflow as tf
import tensorflow_hub as hub
import pandas as pd
from sklearn import preprocessing
import keras
import numpy as np

url="https://tfhub.dev/google/universal-sentence-encoder-large/3"
embed=hub.KerasLayer(url)

from tensorflow.keras.layers import Embedding
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Sequential
from tensorflow.keras.preprocessing.text import one_hot

df.head()



x=list(df['original_text'])
y=list(df['label'])

#encode categorical labels with values between 0 and 1
#convert text label to become numerical label so that it can be used as input for a machine learning model
le = preprocessing.LabelEncoder()
le.fit(y)

y

def encode(le,labels):
  enc=le.transform(labels)
  return keras.utils.np_utils.to_categorical(enc)

def decode(le,one_hot):
  dec=np.argmax(one_hot,axis=1)
  return le.inverse_transform(dec)

x_enc = x
y_enc = encode(le,y)

x_train = np.asarray(x_enc[:1516])
y_train = np.asarray(y_enc[:1516])

x_test=np.asarray(x_enc[1516:])
y_test=np.asarray(y_enc[1516:])

from keras.layers import Input,Lambda, Dense
from keras.models import Model
import keras.backend as K

def UniversalEmbedding(x):
  return embed(tf.squeeze(tf.cast(x,tf.string)))

# input_text = Input(shape=(1,),dtype=tf.string)
# embedding=Lambda(UniversalEmbedding,output_shape=(512,))(input_text)
# dense=Dense(256,activation='relu')(embedding)
# pred=Dense(2,activation='sigmoid')(dense)
# model=Model(inputs=[input_text],outputs=pred)
# model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])
# model.summary()
from keras.losses import categorical_crossentropy

input_text = Input(shape=(1,), dtype=tf.string)
embedding = Lambda(UniversalEmbedding, output_shape=(512,))(input_text)
dense = Dense(256, activation='relu')(embedding)
pred = Dense(2, activation='sigmoid')(dense)
model = Model(inputs=[input_text], outputs=pred)
model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
model.summary()
##This way you can reuse the same architecture for different embedding functions you want to try
#my_model = text_classification_model(UniversalEmbedding)

history=model.fit(x_train,y_train,epochs=10,batch_size=12, validation_data=(x_test, y_test))

predicts=model.predict(x_test,batch_size=12)

y_test=decode(le,y_test)
y_preds=decode(le,predicts)
y_preds

from sklearn import metrics

cm_RF=metrics.confusion_matrix(y_test,y_preds)

print(metrics.classification_report(y_test,y_preds))

import seaborn as sns
import matplotlib.pyplot as plt
#Plotting the confusion matrix
plt.figure(figsize=(8,7))
sns.heatmap(cm_RF, annot=True,cmap="Blues")
plt.title('Confusion Matrix')
plt.ylabel('Actual suicidal note')
plt.xlabel('Predicted suicidal note')
plt.show()

import matplotlib.pyplot as plt
 
loss_train = history.history['loss']
loss_val = history.history['val_loss']
epochs = range(1,11)
plt.plot(epochs, loss_train, 'g', label='Training loss')
plt.plot(epochs, loss_val, 'b', label='validation loss')
plt.title('Training and Validation loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.show()

loss_train = history.history['accuracy']
loss_val = history.history['val_accuracy']
epochs = range(1,11)
plt.plot(epochs, loss_train, 'g', label='Training accuracy')
plt.plot(epochs, loss_val, 'b', label='validation accuracy')
plt.title('Training and Validation accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()
plt.show()



